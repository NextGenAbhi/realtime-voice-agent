<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Assistant</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #2c2c2c;
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
            margin: 0;
            overflow: hidden;
        }
        .animation-container {
            position: relative;
            width: 200px;
            height: 200px;
            display: flex;
            justify-content: center;
            align-items: center;
        }
        .animation-circle {
            position: absolute;
            border-radius: 50%;
            opacity: 0;
            transform: scale(0);
            transition: all 0.5s ease-out;
        }
        .listening-animation {
            width: 100px;
            height: 100px;
            background-color: rgba(52, 152, 219, 0.5);
        }
        .speaking-animation {
            width: 150px;
            height: 150px;
            background-color: rgba(46, 204, 113, 0.5);
        }
        .active {
            opacity: 1;
            transform: scale(1);
            animation: pulse 1.5s infinite ease-in-out;
        }
        @keyframes pulse {
            0% {
                transform: scale(0.8);
                opacity: 0.5;
            }
            50% {
                transform: scale(1.2);
                opacity: 1;
            }
            100% {
                transform: scale(0.8);
                opacity: 0.5;
            }
        }
    </style>
</head>
<body>

<div class="animation-container">
    <div class="animation-circle listening-animation" id="listeningAnimation"></div>
    <div class="animation-circle speaking-animation" id="speakingAnimation"></div>
</div>

<script>
    const listeningAnimation = document.getElementById('listeningAnimation');
    const speakingAnimation = document.getElementById('speakingAnimation');
    
    let websocket;
    let mediaRecorder;
    let audioChunks = [];
    let assistantAudio;
    let assistantState = 'idle'; // 'idle', 'listening', 'speaking'
    let silenceTimeout;
    let audioContext, analyser, microphone;

    function connectWebSocket() {
        websocket = new WebSocket("ws://" + location.host + "/ws/voice");
        websocket.onopen = () => console.log("WebSocket connection established");
        websocket.onmessage = (event) => {
            const data = JSON.parse(event.data);
            if (data.assistant_audio) {
                playAssistantResponse(data.assistant_audio);
            }
        };
        websocket.onclose = () => {
            console.log("WebSocket connection closed. Reconnecting...");
            setTimeout(connectWebSocket, 1000);
        };
        websocket.onerror = (error) => console.error("WebSocket error:", error);
    }

    function playAssistantResponse(audioBase64) {
        stopListening();
        assistantState = 'speaking';
        speakingAnimation.classList.add('active');
        
        assistantAudio = new Audio("data:audio/wav;base64," + audioBase64);
        assistantAudio.play();
        
        assistantAudio.onended = () => {
            assistantState = 'idle';
            speakingAnimation.classList.remove('active');
            startListening();
        };
    }

    function stopAssistant() {
        if (assistantAudio) {
            assistantAudio.pause();
            assistantAudio.currentTime = 0;
        }
        assistantState = 'idle';
        speakingAnimation.classList.remove('active');
    }

    async function initVoiceDetection() {
        try {
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            audioContext = new AudioContext();
            analyser = audioContext.createAnalyser();
            microphone = audioContext.createMediaStreamSource(stream);
            microphone.connect(analyser);
            analyser.fftSize = 512;
            const dataArray = new Uint8Array(analyser.frequencyBinCount);

            mediaRecorder = new MediaRecorder(stream);
            mediaRecorder.ondataavailable = event => audioChunks.push(event.data);
            mediaRecorder.onstop = () => {
                const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
                if (audioBlob.size > 1000) { // only send if there is some audio
                    const reader = new FileReader();
                    reader.readAsDataURL(audioBlob);
                    reader.onloadend = function() {
                        const base64Audio = reader.result.split(',')[1];
                        if (websocket && websocket.readyState === WebSocket.OPEN) {
                            websocket.send(JSON.stringify({audio: base64Audio}));
                        }
                    };
                }
                audioChunks = [];
            };

            detectVoice(dataArray);
        } catch (err) {
            console.error("Error accessing microphone:", err);
        }
    }

    function detectVoice(dataArray) {
        analyser.getByteTimeDomainData(dataArray);
        let sum = 0;
        for(const amplitude of dataArray) {
            sum += Math.pow(amplitude / 128.0 - 1.0, 2);
        }
        const volume = Math.sqrt(sum / dataArray.length);

        if (volume > 0.02) { // Speaking detected
            if (assistantState === 'speaking') {
                stopAssistant();
            }
            if (assistantState !== 'listening') {
                startRecording();
            }
            clearTimeout(silenceTimeout);
            silenceTimeout = setTimeout(stopRecording, 1500);
        }
        requestAnimationFrame(() => detectVoice(dataArray));
    }

    function startRecording() {
        if (mediaRecorder.state !== 'recording') {
            assistantState = 'listening';
            listeningAnimation.classList.add('active');
            mediaRecorder.start();
        }
    }

    function stopRecording() {
        if (mediaRecorder.state === 'recording') {
            mediaRecorder.stop();
            assistantState = 'idle';
            listeningAnimation.classList.remove('active');
        }
    }
    
    function startListening(){
        // dummy function to make it explicit
    }

    function stopListening(){
        // dummy function to make it explicit
    }

    connectWebSocket();
    initVoiceDetection();
</script>

</body>
</html>
